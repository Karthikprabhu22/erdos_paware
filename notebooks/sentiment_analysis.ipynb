{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from adding_metadata.replies import add_reply_list\n",
    "from adding_metadata.reply_sentiments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data\n",
    "##Location of reddit.parquet\n",
    "base_dir = \"../../\" \n",
    "\n",
    "##To store the data splits\n",
    "data_dir = os.path.join(base_dir, \"data/\") \n",
    "\n",
    "##To store results of sentiment analysis\n",
    "results_dir = os.path.join(data_dir, \"results/\") \n",
    "\n",
    "##To update the data splits with sentiment analysis results\n",
    "processed_dir = os.path.join(data_dir, \"processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into parts each with ~50k rows\n",
    "df = pl.read_parquet(os.path.join(base_dir,\"reddit.parquet\"))\n",
    "num_partitions = 100\n",
    "chunk_size = (len(df) + num_partitions - 1) // num_partitions  \n",
    "small_dfs = [df[i:min(i + chunk_size, len(df))] for i in range(0, len(df), chunk_size)]\n",
    "for idx, small_df in enumerate(small_dfs):\n",
    "    output_path = os.path.join(data_dir,f'split_{idx + 1}.parquet')\n",
    "    small_df.write_parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sentiment analysis on each split and save the results\n",
    "all_files = get_all_files(data_dir)\n",
    "for file in all_files:\n",
    "    data = TextLoader(file=file, tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(data, batch_size=50, shuffle=False)\n",
    "    out=[]\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        input = data.to(device_staging)\n",
    "        res = model(input)\n",
    "        out.append(res['logits'].cpu().data)\n",
    "    filename = file.stem\n",
    "    output_file = 'results/' + filename + '.npy'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(pickle.dumps(out))\n",
    "    shutil.move(file, os.path.join(processed_dir, file.name))\n",
    "    del data, train_dataloader, input, res, out\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataframes with sentiments\n",
    "for idx in range(num_partitions):\n",
    "    parquet_path = f\"{processed_dir}/split_{idx + 1}.parquet\"\n",
    "    npy_file_path = f\"{results_dir}/split_{idx + 1}.npy\"\n",
    "    update_dataframe_with_sentiments(base_dir, parquet_path, npy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine updated split parquet files into one\n",
    "combine_parquet_files(processed_dir, f\"{base_dir}/reddit_updated_with_sentiments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the updated DataFrame and add reply list and summed sentiments\n",
    "df_new = pl.read_parquet(f\"{base_dir}/reddit_updated_with_sentiments.parquet\")\n",
    "df_new = replies.add_reply_list(df_new)\n",
    "df_new = add_summed_sentiments(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame with summed sentiments\n",
    "df_new.write_parquet(f\"{base_dir}/reddit_updated_with_sentiments.parquet\", compression='zstd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
