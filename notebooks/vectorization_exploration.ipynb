{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a27048a-1316-4c69-b9dd-b0358b720a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import polars as pl\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "DATA_PARQUET = \"../temp_data/raw_data_subset_gm.parquet\"\n",
    "DATA_PARQUET = \"../temp_data/raw_data_subset_ups_fedex_lowes.parquet\"\n",
    "DATA_PARQUET = \"../temp_data/raw_data_subset_ups_fedex_lowes.parquet\"\n",
    "## Set embedding model\n",
    "## https://huggingface.co/thenlper/gte-base\n",
    "EMBEDDINGS_MODEL = \"thenlper/gte-base\"\n",
    "\n",
    "## Load data from parquet\n",
    "DATA_RAW = pl.read_parquet(DATA_PARQUET)[0:1000]\n",
    "##DATA_RAW = pl.read_parquet(DATA_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb30e75-ebf1-46f6-8857-fd3f5ff0f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(DATA_RAW: pl.DataFrame)->pl.DataFrame:\n",
    "    '''\n",
    "    Complete basis preprocessing tasks:\n",
    "        - Drop rows with comments|submissions that have been dropped|deleted\n",
    "        - Drop rows with comments|submissions automatically generated by a bot\n",
    "        - Drop rows with comments|submissions that are empty\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        DATA_RAW: pl.DataFrame\n",
    "            A dataframe loaded from the raw Aware data provided\n",
    "    '''\n",
    "    \n",
    "    ## Clone raw data\n",
    "    data_preprocessed = DATA_RAW.clone()\n",
    "    \n",
    "    ## Drop rows with comments|submissions that have been dropped|deleted\n",
    "    values_to_drop = [\"[deleted]\", \"[removed]\"]\n",
    "    for value in values_to_drop:\n",
    "        mask = (data_preprocessed[\"reddit_text\"] != value)\n",
    "        print(f\"Dropping {data_preprocessed.filter(~mask).shape[0]} rows with \"+\n",
    "              f\"reddit_text=='{value}'\")\n",
    "        data_preprocessed = data_preprocessed.filter(mask)\n",
    "\n",
    "    ## Drop rows with comments|submissions automatically generated by a bot\n",
    "    pattern =(r\"This has been removed for breaking the sub rule of\")\n",
    "    mask = data_preprocessed[\"reddit_text\"].str.contains(pattern)\n",
    "    print(f\"Dropping {data_preprocessed.filter(mask).shape[0]} rows with \"+\n",
    "          \"reddit_text containing:\"+\n",
    "          \"'This has been removed for breaking the sub rule of'\")\n",
    "    data_preprocessed = data_preprocessed.filter(~mask)\n",
    "\n",
    "    ## Drop rows with comments|submissions that are empty\n",
    "    mask = (data_preprocessed[\"reddit_text\"]==\"\")\n",
    "    print(f\"Dropping {data_preprocessed.filter(mask).shape[0]} rows with \"+\n",
    "              f\"reddit_text==''\")\n",
    "    data_preprocessed = data_preprocessed.filter(~mask)\n",
    "    \n",
    "    return data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf8b123-8818-4205-b064-dcf2c136ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_preprocessed_data(data_preprocessed: pl.DataFrame,\n",
    "                            chunk_size: int,\n",
    "                            chunk_overlap_pct: float)->pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a pl.DataFrame with reddit data, break target data into chunks\n",
    "    suitable for embedding.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_raw: pl.DataFrame\n",
    "        A dataframe containing the raw data\n",
    "    chunk_size: int\n",
    "        The maximum size of any text chunk\n",
    "    chunk_overlap_pct: float\n",
    "        The percent adjacent text chunks should overlap\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame\n",
    "        A dataframe with a new column:\n",
    "            - \"text_chunk\": A chunk of text determined by the splitter                          \n",
    "    \"\"\"\n",
    "    data_chunked = data_preprocessed.clone()\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=int(chunk_size*chunk_overlap_pct),\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "            strip_whitespace=True)\n",
    "    \n",
    "    ## Break longer texts into chunks\n",
    "    data_chunked = data_chunked.with_columns(\n",
    "        text_chunk=pl.col(\"reddit_text\").map_elements(\n",
    "            lambda x:text_splitter.split_text(x)))\n",
    "    ## Explode the text chunks\n",
    "    data_chunked = data_chunked.explode(\"text_chunk\")\n",
    "\n",
    "    mask = (data_chunked[\"text_chunk\"].is_null())\n",
    "    print(f\"There are {data_chunked.filter(mask).shape[0]} rows with \"+\n",
    "                  f\"text_chunk is null\")\n",
    "    ## Return the chunked dataframe\n",
    "    return data_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24c4555-0043-481f-872a-141531c2ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_chunked_data(data_chunked: pl.DataFrame)->pl.DataFrame:\n",
    "    '''\n",
    "        Vectorize the chunked texts\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            data_chunked: pl.DataFrame\n",
    "                A dataframe with a chunk of text to be embedded\n",
    "\n",
    "        Returns:\n",
    "            pl.DataFrame\n",
    "                A datafram with a new column:\n",
    "                    - vector\n",
    "                        Contains the vector representing the text_chunk\n",
    "                        determined by the EMBEDDINGS_MODEL\n",
    "    '''\n",
    "    ## Set up model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)\n",
    "    data_vectorized = data_chunked.clone()\n",
    "    \n",
    "    ## Remove null chunks\n",
    "    mask = (data_vectorized[\"text_chunk\"].is_null())\n",
    "    print(f\"Dropping {data_vectorized.filter(mask).shape[0]} rows with \"+\n",
    "                  f\"text_chunk is null\")\n",
    "    data_vectorized = data_vectorized.filter(~mask)\n",
    "    \n",
    "    ## Compute embeddings\n",
    "    text_chunks = data_vectorized[\"text_chunk\"].to_list()\n",
    "    vectors = embeddings.embed_documents(text_chunks)\n",
    "    \n",
    "    ## Add embeddings to the dataframe\n",
    "    data_vectorized = data_vectorized.with_columns(\n",
    "        pl.Series(\n",
    "            name=\"vector\",\n",
    "            values=vectors\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return data_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5193d1c6-fe0a-45ae-8b66-ab33c6458a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 3 rows with reddit_text=='[deleted]'\n",
      "Dropping 2 rows with reddit_text=='[removed]'\n",
      "Dropping 1 rows with reddit_text containing:'This has been removed for breaking the sub rule of'\n",
      "Dropping 5 rows with reddit_text==''\n",
      "There are 0 rows with text_chunk is null\n",
      "CPU times: user 15.9 ms, sys: 11.9 ms, total: 27.8 ms\n",
      "Wall time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_preprocessed = preprocess_data(DATA_RAW=DATA_RAW)\n",
    "data_chunked = chunk_preprocessed_data(data_preprocessed=data_preprocessed,\n",
    "                                       chunk_size=512,\n",
    "                                       chunk_overlap_pct=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44c46b17-1fb1-42ee-b82e-656b730c5eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 15)\n",
      "(989, 15)\n",
      "(1098, 16)\n"
     ]
    }
   ],
   "source": [
    "print(DATA_RAW.shape)\n",
    "print(data_preprocessed.shape)\n",
    "print(data_chunked.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1bf24e3-fca6-4987-bc14-19b0c0c7b27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 rows with text_chunk is null\n",
      "CPU times: user 3.86 s, sys: 1.88 s, total: 5.74 s\n",
      "Wall time: 5.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_vectorized = vectorize_chunked_data(data_chunked=data_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc28d3b5-adfd-4b95-99c2-87a72c774130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1098, 17)\n"
     ]
    }
   ],
   "source": [
    "print(data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cecc714e-58c2-4890-a8f1-c2000aa1b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_vectorized.write_parquet(\"temp_data/reddit_subset_gm_vectorized.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21be0edc-0a6a-4a29-8cbb-0b9a40d3fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)\n",
    "\n",
    "def query_results(query, num_results):\n",
    "    \n",
    "    query_vector = embeddings.embed_query(query)   \n",
    "    df = data_vectorized.clone()\n",
    "    \n",
    "    similarity_list = []\n",
    "    for vector in df[\"vector\"]:\n",
    "        similarity_list.append(cosine(vector,query_vector))\n",
    "    \n",
    "    df = df.with_columns(pl.Series(name=\"similarity\",values=similarity_list))\n",
    "    \n",
    "    top_results = df.sort(pl.col(\"similarity\"), \n",
    "                          descending=False).head(num_results)[\"reddit_text\"]\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    for result in top_results:\n",
    "        print(\"-\"*80)\n",
    "        print(result)\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46f8f5b-2bee-44ed-af9b-5850728d160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many days off do you get in a year?\n",
      "--------------------------------------------------------------------------------\n",
      "It another bonus some people get to get during the year. Can be either 3 or 5 years in length to get the full payout.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Outlook and Teams don't tell a worker they have 3 hours to get a task done. Most people have at least a few meetings and other obligations throughout the day which require them to be aware of time and check the calendar. It's not just work but picking the kids up from soccer and putting them to bed. \n",
      "\n",
      "An extra workday per week is more than an hour and a half per day. An increase approaching 20%.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "145%  Enough to make people happy but not over 150% to make The Board question. Although I would like the 200% suggested. Might be a bit higher to offset some of the chaos of dictated days for RTO with little notice. Even people who were coming in are losing flexibility to choose the days.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "We are at will employees and you will eventually get fired. In the Austin Innovation center we have been mandatory 3 days a week in the office since last spring/early summer.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "This is a clear sign that they want to reduce headcount without layoffs by making people angry enough to quit.  And if this doesn't work, they'll make it more annoying.\n",
      "\n",
      "Here's my story (and saying vague so as to not doxx myself).\n",
      "\n",
      "The backstory:A pre-GM employer of mine did something similar to my team. I was hired as hybrid (before that term existed) salaried employee in a software & engineering support function. This is how they got me to quit rather than be laid off (they ended up doing limited layoffs about 4 months later, but by then I already had another role).\n",
      "\n",
      "1-  (late Feb/early March) I was reclassified as hourly. This meant no more remote work, since hourly employees that weren't field employees were required to work in-office. I was told to continue working like I was, just from the office. \n",
      "\n",
      "2- A month later, they restricted my hours to no more than 40. As salaried employee I worked 50+ hours each week, so when they reclassified me, I started to get overtime.  Meanwhile, my responsibilities weren't reduced - so I had 20% less time to do the same amount of work. \n",
      "\n",
      "3- A month later (remember, software function), they added a completely unrelated sales triage support function to my responsibilities. I was completely unprepared and unqualified for this. This was on top of my existing responsibilities.\n",
      "\n",
      "4- A month later, midyear review. By now, my performance had begun to slip. I asked for help, it was refused and I made efforts to transfer to a different role more suited to my qualifications, but couldn't even get an interview. I painted my results in the best light possible, but with my slipping performance, I was put in a 90-day PIP.\n",
      "\n",
      "5- Instead I started to search for work and found a role with a customer of ours within a few weeks. I left my two week's notice, and in those two weeks, I made sure to \"work\" as much overtime as possible as a final fuck you. That final paycheck was glorious.\n",
      "\n",
      "6- Notes. Throughout this entire time, they also changed my cubicle (and many others's) several times with little notice and no reason given, so more aggravation as well. But the following year, because I had worked JUST enough hours thanks to that bit of overtime, I still received a prorated bonus the following year.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "lol no one is licking boots again it’s called the reality of the situation. I work in the field so 10 hours is a joke. I WFH a majority of the time and will continue so because that is my job. PTO are having your phone and laptop still with you because you never know, holidays are “optional” because if the dealerships are open then we’re open. The big word you missed is “if.” If you worked your normal shift then good for you no projecting here but you guys can’t work without trying to play the blame game and actually look at reality of not just GM but corporate America as a whole (everyone is being asked to RTO).\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "My strategy is to skip Wednesdays until Feb 28. After that I don’t care. My 401k vests interim and I will have a year before they fire me or GM- comes into question. Enough time to find another job.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "You are the typical “if you let them do this then what’s next” type of person. In office yes those breaks don’t account for much I agree, but being at your house all day I’d be willing to bet a substantial amount it’s not a couple of minutes (again unless you are the outlier in this situation). The statistics come from the majority not the outlier. YOU may have been extremely efficient working at home and in the office. Congratulations you aren’t the reason RTO is being strongly enforced and probably are like me to where you don’t need rules spoon fed to you daily. The real reasoning is coming from the majority of turning in subpar work because they are screwing around same with dumbass rules that should just be common sense.\n",
      "\n",
      "I’m on your side believe me but reality is the focus here.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "It will be up to your manager since they are leaving up to individual teams/orgs to police who are following or not following the RTO policies. \n",
      "\n",
      "If your manager is flexible about what days you are in the office then as long as you're discussing that expectation with them then likely no repercussions. If they are insisting on 3 days a week then better start looking for a new job.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Oh and 4, worked far more hours from home than I did pre COVID, due to working instead of driving, through lunches etc. which is the case for most of the employees\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 328 ms, sys: 38.6 ms, total: 367 ms\n",
      "Wall time: 369 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_results(\"How many days off do you get in a year?\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da148fa6-80b5-4ab4-81e0-cf975242916d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aware] *",
   "language": "python",
   "name": "conda-env-aware-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
